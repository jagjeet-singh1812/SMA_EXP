Artificial intelligence (AI) is the development of computer systems that are able to perform tasks that normally require human intelligence. Advances in AI software and hardware, especially deep learning algorithms and the graphics processing units (GPUs) that power their training, have led to a recent and rapidly increasing interest in medical AI applications. In clinical diagnostics, AI-based computer vision approaches are poised to revolutionize image-based diagnostics, while other AI subtypes have begun to show similar promise in various diagnostic modalities. In some areas, such as clinical genomics, a specific type of AI algorithm known as deep learning is used to process large and complex genomic datasets. In this review, we first summarize the main classes of problems that AI systems are well suited to solve and describe the clinical diagnostic tasks that benefit from these solutions. Next, we focus on emerging methods for specific tasks in clinical genomics, including variant calling, genome annotation and variant classification, and phenotype-to-genotype correspondence. Finally, we end with a discussion on the future potential of AI in individualized medicine applications, especially for risk prediction in common complex diseases, and the challenges, limitations, and biases that must be carefully addressed for the successful deployment of AI in medical applications, particularly those utilizing human genetics and genomics data. 

Mimicking human intelligence is the inspiration for AI algorithms, but AI applications in clinical genomics tend to target tasks that are impractical to perform using human intelligence and error prone when addressed with standard statistical approaches. Many of the techniques described above have been adapted to address the various steps involved in clinical genomic analysis—including variant calling, genome annotation, variant classification, and phenotype-to-genotype correspondence—and perhaps eventually they can also be applied for genotype-to-phenotype predictions. Here, we describe the major classes of problems that have been addressed by AI in clinical genomics.
Variant calling
The clinical interpretation of genomes is sensitive to the identification of individual genetic variants among the millions populating each genome, necessitating extreme accuracy. Standard variant-calling tools are prone to systematic errors that are associated with the subtleties of sample preparation, sequencing technology, sequence context, and the sometimes unpredictable influence of biology such as somatic mosaicism [50]. A mixture of statistical techniques including hand-crafted features such as strand-bias [51] or population-level dependencies [52] are used to address these issues, resulting in high accuracy but biased errors [53]. AI algorithms can learn these biases from a single genome with a known gold standard of reference variant calls and produce superior variant calls. DeepVariant, a CNN-based variant caller trained directly on read alignments without any specialized knowledge about genomics or sequencing platforms, was recently shown to outperform standard tools on some variant-calling tasks [54]. The improved accuracy is thought to be due to the ability of CNNs to identify complex dependencies in sequencing data. In addition, recent results suggest that deep learning is poised to revolutionize base calling (and as a result, variant identification) for nanopore-based sequencing technologies, which have historically struggled to compete with established sequencing technology because of the error-prone nature of prior base-calling algorithms [55].
Genome annotation and variant classification
After variant calling, the interpretation of human genome data relies on the identification of relevant genetic variants through prior knowledge and inference of the impact of genetic variants on functional genomic elements. AI algorithms can improve the use of prior knowledge by informing phenotype-to-genotype mapping (described in the next section). Here, we describe both genome annotation and variant classification because many of the AI algorithms that are used to predict the presence of a functional element from primary DNA sequence data are also used to predict the impact of a genetic variation on those functional elements.
Classification of coding variants
Many methods have been developed for the classification of nonsynonymous variants [56]. Some of these methods have been integrated into deep-learning-based meta-predictors (models that process and merge the predictions produced by several other predictors) that outperform both their individual predictive components and the combination of those predictive components when integrated using regression or other machine-learning approaches [57]. For example, the combined annotation-dependent depletion approach (CADD) [58] combines a variety of predictive features in a machine-learning algorithm to predict the deleteriousness of genetic variants. A deep-learning-based extension of CADD, named DANN, demonstrated improved performance using the same set of input features as CADD but combined in a deep neural network [57]. This technical extension of CADD suggests that deep learning may be a superior approach for integrating known features that are predictive of deleteriousness. However, the classification accuracies of these tools are not sufficient to drive clinical reporting, although they can be useful for guiding the interpretation of clinical genomic data by prioritizing potential candidate variants for further consideration.
More interesting are AI-based methods that make predictions directly from DNA or protein sequence data with minimal hand-crafting of features. One approach, PrimateAI, which used CNNs trained on variants of known pathogenicity with data augmentation using cross-species information, was shown to outperform prior methods when trained directly upon sequence alignments [59]. The network was able to learn important protein domains, conserved amino acid positions, and sequence dependencies directly from the training data consisting of about 120,000 human samples. PrimateAI substantially exceeded the performance of other variant pathogenicity prediction tools in differentiating benign and pathogenic de-novo mutations in candidate developmental disorder genes, and in reproducing prior knowledge in Clinvar [60]. These results suggest that PrimateAI is an important step forward for variant-classification tools that may lessen the reliance of clinical reporting on prior knowledge. In addition, deep generative models have shown promise for predicting the effects of genetic variants [61], and are especially intriguing given their ability to evaluate the joint influence of multiple genetic variants and/or complex indels on protein function, a capability that is largely absent from most pathogenicity prediction tools. Deep generative models are a type of deep neural network that can learn to replicate data distributions and produce examples not previously observed by the model. For example, a deep generative model trained on images of birds could learn to generate novel bird images.
Classification of non-coding variants
The computational identification and prediction of non-coding pathogenic variation is an open challenge in human genomics [62]. Recent findings suggest that AI algorithms will substantially improve our ability to understand non-coding genetic variation. Splicing defects in genes are responsible for at least 10% of rare pathogenic genetic variation [63], but they can be difficult to identify because of the complexity of intronic and exonic splicing enhancers, silencers, insulators, and other long range and combinatorial DNA interactions that influence gene splicing [64]. SpliceAI, a 32-layer deep neural network, is able to predict both canonical and non-canonical splicing directly from exon–intron junction sequence data [27]. Remarkably, SpliceAI was able to use long-range sequence information to boost prediction accuracy from 57%, using a short window size (80 nucleotides) typical for many prior splicing prediction tools, to 95% when a 10 kb window size was ingested by the AI algorithm, and was able to identify candidate cryptic splicing variants underlying neurodevelopmental disorders.
Deep-learning-based approaches have also substantially improved our ability to detect regulatory elements [65, 66] and to predict the influence of genetic variation on those elements. DeepSEA, a multitask hierarchically structured CNN trained on large-scale functional genomics data [67], was able to learn sequence dependencies at multiple scales and simultaneously produce predictions of DNase hypersensitive sites, transcription factor binding sites, histone marks, and the influence of genetic variation on those regulatory elements, with a level of accuracy superior to those of other tools for prioritizing non-coding functional variants [68]. As seen for SpliceAI, the ability of DeepSEA to ingest DNA sequences of 1 kb, which is substantially larger than the input to typical motif-based search tools, was critical to this improved performance. Extensions of DeepSEA have been applied to whole-genome sequencing data from families with autism spectrum disorder to reveal several candidate non-coding mutations [69]. Further extension to the ExPecto algorithm has demonstrated its ability to predict gene expression levels directly from DNA sequence information [70]. Further investigation of these new deep-learning based frameworks for the analysis of non-coding sequence data is likely to provide new insights into the regulatory code of the human genome.
Phenotype-to-genotype mapping
Human genomes contain numerous genetic variants that are either previously described as pathogenic or predicted to be pathogenic [71], regardless of the individual health status [72]. Therefore, the molecular diagnosis of disease often requires both the identification of candidate pathogenic variants and a determination of the correspondence between the diseased individual’s phenotype and those expected to result from each candidate pathogenic variant. AI algorithms can significantly enhance the mapping of phenotype to genotype, especially through the extraction of higher-level diagnostic concepts that are embedded in medical images and EHRs.
Image to genetic diagnosis
The human phenotype ontology lists 1007 distinct terms defining different abnormalities of the face [73]. These abnormalities are associated with 4526 diseases and 2142 genes. A dysmorphologist will often identify these abnormalities individually and synthesize them into a clinical diagnosis. The clinical diagnosis may then inform targeted gene sequencing or phenotype-informed analysis of more comprehensive genetic data. Often the human-provided clinical diagnosis and molecular diagnoses overlap but do not match precisely because of the phenotypic similarity of genetically distinct syndromes. DeepGestalt, a CNN-based facial image analysis algorithm, dramatically outperforms human dysmorphologists in this task and is precise enough to distinguish between molecular diagnoses that are mapped to the same clinical diagnosis (that is, distinct molecular forms of Noonan syndrome) [19]. When combined with genomic data, PEDIA, a genome interpretation system incorporating DeepGestalt, was able to use phenotypic features extracted from facial photographs to accurately prioritize candidate pathogenic variants for 105 different monogenic disorders across 679 individuals [74]. Deployment of DeepGestalt as a face-scanning app has the potential to both democratize and revolutionize the identification of genetic syndromes [20].
Genetic syndromes that are identified through facial analysis can be readily confirmed with DNA testing, but adequate material for somatic mutation testing is not always available in some instances of cancer. Nevertheless, knowledge of the genomic underpinnings of a tumor are critical to treatment planning. Here again, AI can bridge the gap between image-derived phenotypes and their probable genetic source. A ‘survival CNN’, which is a combination of a CNN with Cox proportional hazards-based outcomes (a type of statistical survival analysis), was able to learn the histological features of brain tumors that are associated with survival and correlated with somatic mutation status [75]. Importantly, this algorithm was not trained to predict genomic aberrations directly. Inspection of the CNN concepts used to make the survival predictions identified novel histological features that are important for prognosis determination. Like the faces of individuals with phenotypically overlapping genetic syndromes, these results suggest that the genomic aberrations underpinning an individual’s tumor could potentially be predicted directly from tumor histology images. More generally, AI-based computer vision systems appear to be capable of predicting the genomic aberrations that are likely to be present in an individual’s genome on the basis of the complex phenotypes embedded in relevant clinical images 

AI Physics bridges the gap between artificial intelligence and the laws governing the natural world, offering the promise of deeper insights and novel applications across the realm of physics. Both fields underscore the importance of interdisciplinary collaboration and innovation in shaping the future of technology and scientific exploration.

It aims to apply AI techniques to various areas of physics, from analyzing complex physical systems to enhancing our understanding of fundamental laws of the universe. 

AI physics leverages machine learning algorithms to simulate, predict, and optimize physical phenomena. Neural networks and other AI models can aid in solving complex differential equations, predicting quantum mechanical behaviors, and even optimizing experimental setups. 

This fusion of AI and physics has the potential to accelerate scientific discoveries, streamline data analysis in particle physics and astronomy, and lead to innovations in materials science and energy research. As AI physics evolves, it also raises philosophical questions about the nature of intelligence, the relationship between machine and human understanding of the universe, and the potential insights that machines might uncover in the realm of fundamental physics.

In this section, we’ll explore the tangible ways AI is being used in the field of physics. From simulating intricate physical systems to analyzing massive datasets in high-energy physics experiments, AI is accelerating discoveries and pushing the boundaries of our understanding. We’ll also delve into the exciting realm of quantum computing and its intersection with AI, paving the way for quantum advancements that were once thought impossible.

AI has revolutionized the field of physics by introducing innovative applications that enhance our understanding of complex physical phenomena and accelerate the discovery process. 

One prominent application is in the realm of particle physics, where AI-driven techniques aid in the analysis of massive datasets generated by particle colliders like the Large Hadron Collider (LHC). Machine learning algorithms excel at identifying elusive particle signatures amidst noisy data, enhancing the accuracy of particle identification and enabling the discovery of rare events that could potentially unveil new fundamental particles or interactions.

Another promising application is computational physics and materials science. AI-driven simulations are transforming the way researchers model and predict the behavior of materials at various scales, from atomic interactions to macroscopic properties. These simulations not only save time but also offer insights into the properties of materials that are challenging to investigate experimentally. 

Such AI-powered simulations have contributed to the design of novel materials with tailored properties for applications in electronics, energy storage, and even drug discovery.

Furthermore, AI plays a crucial role in advancing astrophysics by analyzing the vast amount of data collected from telescopes and space missions. Machine learning techniques aid in the identification of celestial objects, the characterization of their properties, and the discovery of new astronomical phenomena. 

These algorithms help astronomers sift through enormous datasets, identifying rare events such as gravitational waves or transient cosmic events. AI also contributes to the development of sophisticated models for cosmological simulations, allowing researchers to better understand the evolution of the universe and the distribution of dark matter and dark energy. As AI continues to evolve, its applications in physics promise to unravel deeper mysteries of the universe and revolutionize the way we conduct scientific research.

Nature has been an incredible source of inspiration for AI algorithms. In this section, we’ll uncover the algorithms that draw inspiration from physics principles. Genetic algorithms, swarm intelligence, and neural architectures modeled after the human brain showcase the remarkable synergy between AI engineering and physics concepts.

Physics-inspired AI algorithms are a captivating intersection of two distinct yet harmonious realms of knowledge, harnessing the principles that govern the physical universe to enhance artificial intelligence techniques. These algorithms draw inspiration from fundamental concepts in physics, such as optimization, entropy, and conservation laws, to develop novel and efficient solutions to complex problems. 

By imbuing AI systems with the elegance and predictive power inherent in the laws of physics, researchers aim to create more adaptable, robust, and interpretable algorithms.

One prime example of physics-inspired AI algorithms is the field of quantum machine learning. By leveraging principles from quantum mechanics, these algorithms exploit the inherent parallelism and superposition properties of quantum states to potentially outperform classical machine learning techniques in specific applications. 

Quantum-inspired neural networks, for example, can process information in a highly parallel manner, mirroring the behavior of quantum systems, to accelerate tasks like optimization and pattern recognition. 

The marriage of quantum physics and AI promises to usher in a new era of computational capabilities, revolutionizing industries such as cryptography and drug discovery, as well as optimization problems that classical computers struggle to solve efficiently.

Physics-inspired AI algorithms also find a home in swarm intelligence and evolutionary computing. Drawing inspiration from the collective behaviors observed in natural systems, these algorithms emulate phenomena like flocking birds, schooling fish, and foraging ants. 

By simulating interactions between individual agents and their environment, these algorithms optimize for emergent behaviors, allowing for efficient problem-solving in scenarios where traditional methods might falter. Such approaches hold promise in tasks like optimization, routing and resource allocation, where decentralized decision-making and adaptability are key factors.

Ultimately, the marriage of physics and AI not only enriches our understanding of both domains but also opens up new avenues for solving intricate challenges across a wide spectrum of disciplines.